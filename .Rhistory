library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
library(datasets)
head(iris)
clc
library(datasets)
head(iris)
library(datasets)
head(iris,20)
structure(iris)
structure(iris)
summary(iris)
structure(iris)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + geom_point()
set.seed(20)
irisCluster <- kmeans(iris[, 3:4], 3, nstart = 20)
irisCluster
table(irisCluster$cluster, iris$Species)
irisCluster$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point()
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
?aes
library(ggplot2)
iris.3means$cluster <- as.factor(iris.3means$cluster)
ggplot(iris,aes(iris$Petal.Length,iris$Petal$Width,color=iris.3means$cluster))+geom_point()
irisCluster$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point()
iris [, -5]
iris.kmeans <- list()
for (k in 2:10)
iris.kmeans[[k]] <- kmeans(iris[,-5],k,nstart = 30)
plot(iris.kmeans[[5]]$Petal.Length,iris.kmeans[[5]]$Petal.Width,col=iris$Species)
iris.kmeans <- list()
iris.kmeans[[5]]
for (k in 2:10)
iris.kmeans[[k]] <- kmeans(iris[,-5],k,nstart = 30)
iris.kmeans[[5]]
iris.kmeans[[5]]$cluster
inertie.expl <- rep(0,times=10)
for (k in 2:10){
clus <- kmeans(iris[,-5],centers=k,nstart=5)
inertie.expl[k] <- clus$betweenss/clus$totss
}
#graphique
plot(1:10,inertie.expl,type="b",xlab="Nb. de groupes",ylab="% inertie expliquée")
library(MASS)
library(cluster)
source('../randIndex.R')
data(iris)
fleurs <- NULL
fleurs$num <- iris[, c(1:4)]
fleurs$cls <- iris[, 5]
#PCA
# center fleurs$num by column
fleurs$numCentered <- apply(fleurs$num, 2, function(c) { c - mean(c) })
# covariance matrix
fleurs$sigma <- 1 / dim(fleurs$num)[1] *
t(fleurs$numCentered) %*% fleurs$numCentered
# computes the eigen values and vectors
fleurs$eigen <- eigen(fleurs$sigma)
# principal components
fleurs$principalComponents <- fleurs$numCentered %*% fleurs$eigen$vectors
# computes the percentage of explained inertia for the first factorial plan
fleurs$inertia <- 100 * sum(fleurs$eigen$values[c(1, 2)]) /
sum(fleurs$eigen$values[fleurs$eigen$values > 0])
cat('Pourcentage d\'inertie expliquée par le premier plan factoriel',
fleurs$inertia, '\n')
for (i in 2:4) {
kmeansi <- kmeans(fleurs$num, i)
pngName <- paste('kmeans', i, '.png', sep = '')
png(pngName)
plot(fleurs$principalComponents,
pch = c(1:3)[fleurs$cls],
col = c(1:i)[kmeansi$cluster],
main = paste('Représentation des clusters produits par\n',
'la méthode des centres mobiles avec', i, 'classes'),
xlab = 'Composante 1',
ylab = 'Composante2')
legend(-2, 1.4,
title = 'Classes réelles',
c('Classe 1', 'Classe 2', 'Classe 3'),
pch = c(1, 2, 3))
clusterNames <- c()
clusterCol <- c()
for (k in 1:i) {
clusterNames[k] <- paste('Cluster', k)
clusterCol[k] <- k
}
legend(-0.1, 1.4,
title = 'Clusters issus des kmeans',
clusterNames,
col = clusterCol,
lwd = 1)
dev.off()
cat(pngName, 'sauvegardée\n')
}
inertia <- c()
for (i in 1:100) {
kmeans3 <- kmeans(fleurs$num, 3)
inertia[i] <- kmeans3$tot.withinss
}
print(table(inertia))
inertia <- matrix(0, ncol = 4, nrow = 100)
for (i in 2:5) {
for (j in 1:100) {
kmeansij <- kmeans(fleurs$num, i)
inertia[j, i - 1] <- kmeansij$tot.withinss
}
}
inertiaMeans <- colMeans(inertia)
cat("inerties moyennes:", inertiaMeans, '\n')
pngName <- 'methodeCoude.png'
png(pngName)
plot(c(2, 3, 4, 5), inertiaMeans,
type = 'o',
ylim = c(0, max(inertiaMeans) + 10),
xlab = 'Nombre de classes k',
ylab = 'Inertie intra-classe',
main = 'Inerties intra-classes moyennes pour k = {2, 3, 4, 5} classes')
dev.off()
cat(pngName, 'sauvegardée\n')
randIndex <- randindex(kmeans3$cluster, fleurs$cls)
cat('rand index:', randIndex$rate, '\n')
clus$betweenss
clus$totss
clus$betweenss/clus$totss
sum(clus)
size(clus)
clus
?rep
inertie.expl <- rep(0,times=10)
inertie.expl
dim (jeu1)
jeu1 <- read.table(file.choose(),header = T)
dim (jeu1)
plot(jeu1)
names(jeu1)
head(jeu1)
structure(jeu1)
jeu1
jeu1
iris[,-5]
iris[,3:4]
library(rpart)
tennis <- read.table("http://www.grappa.univ-lille3.fr/~gilleron/jeuxdonnees/tennum.txt")
tennis
ad.tennis <- rpart (Jouer ~ Ciel + Temperature + Humidite + Vent, tennis)
ad.tennis
ad.tennis.cnt <- rpart.control (minsplit = 1)
ad.tennis <- rpart (Jouer ~ Ciel + Temperature + Humidite + Vent, tennis, control = ad.tennis.cnt)
ad.tennis
plot (ad.tennis)
text (ad.tennis)
plot (ad.tennis, uniform=T)
text (ad.tennis, use.n=T, all=T)
plot (ad.tennis, branch=0)
plot (ad.tennis, branch=.7)
text (ad.tennis, use.n=T)
plot (ad.tennis, branch=.4, uniform=T, compress=T)
text (ad.tennis, all=T,use.n=T)
plot (ad.tennis, branch=.2, uniform=T, compress=T, margin=.1)
text (ad.tennis, all=T, use.n=T, fancy=T)
data(iris)
head(iris)
library(tree)
arbre1 <- tree(Species ~ Sepal.Width + Petal.Width, data = iris)
summary(arbre1)
plot(arbre1)
text(arbre1)
plot(iris$Petal.Width,iris$Sepal.Width,pch=19,col=iris$Species)
partition.tree(arbre1,label="Species",add=TRUE)
legend(cex = 1,1.75,4.5,legend=unique(iris$Species),col=unique(as.numeric(iris$Species)),pch=19)
data(iris)
head(iris)
library(tree)
arbre1 <- tree(Species ~ Petal.Length + Petal.Width, data = iris)
summary(arbre1)
require(tree)
install.packages("tree")
library(tree)
arbre1 <- tree(Species ~ Petal.Length + Petal.Width, data = iris)
summary(arbre1)
plot(arbre1)
text(arbre1)
plot(iris$Petal.Width,iris$Petal.Length,pch=19,col=iris$Species)
partition.tree(arbre1,label="Species",add=TRUE)
legend(cex = 1,1.75,4.5,legend=unique(iris$Species),col=unique(as.numeric(iris$Species)),pch=19)
credit <- read.csv("credit.csv")
credit <- read.csv("C:\\Users\\pc\\Desktop\\master2\\Feuille de Données et  Apprentissage Automatique\\Cours et TPs DM (Fi3i 2017)\\TP 4\\credit.csv")
str(credit)
names(credit)
table(credit$checking_balance)
table(credit$savings_balance)
table(credit$checking_balance)
credit$checking_balance
table(credit$checking_balance)
table(credit$savings_balance)
credit$checking_balance
summary(credit$months_loan_duration)
#5. Afficher les statistiques de amount.
summary(credit$amount)
names(credit)
install.packages("C50")
# Charger les données
credit <- read.csv("credit.csv")
str(credit)
# Let's take a look at the table() output for a couple of loan features that seem likely to predict a default. The applicant's checking and savings account balance are recorded as categorical variables:
# variable catégorielle
table(credit$checking_balance)
table(credit$savings_balance)
# Variable numérique
summary(credit$months_loan_duration)
summary(credit$amount)
# Data preparation - creating random training and test datasets
# As we have done in the previous chapters, we will split our data into two portions: a training dataset to build the decision tree and a test dataset to evaluate the performance of the model on new data. We will use 90 percent of the data for training and 10 percent for testing, which will provide us with 100 records to simulate new applicants.
# The following commands use the sample() function to select 900 values at random out of the sequence of integers from 1 to 1000. Note that the set.seed() function uses the arbitrary value 123.
set.seed(123)
train_sample <- sample(1000, 900)
# As expected, the resulting train_sample object is a vector of 900 random integers:
str(train_sample)
# By using this vector to select rows from the credit data, we can split it into the 90 percent training and 10 percent test datasets we desired.
credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]
# If all went well, we should have about 30 percent of defaulted loans in each of the datasets:
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
# Step 3 - training a model on the data
# Nous allons utiliser l'algorithme C5.0 de la bibliothèque c50 qu'il faut au préalable installer et charger
install.packages("C50")
library(C50)
# For the first iteration of our credit approval model, we'll use the default C5.0 configuration, as shown in the following code.
#The 21st column in credit_train is the default class variable,
# so we need to exclude it from the training data frame, but supply it as the target factor vector for classification:
credit_model <- C5.0(credit_train[-21], factor(ifelse(credit_train$default==1,"No","Yes")))
credit_model
summary(credit_model)
credit_pred <- predict(credit_model, credit_test)
install.packages("gmodels")
library(gmodels)
CrossTable(credit_test$default, credit_pred,
prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
dnn = c('actual default', 'predicted default'))
credit
names(credit)
set.seed(123)
set.seed(123)
train_sample <- sample(1000, 900)
# As expected, the resulting train_sample object is a vector of 900 random integers:
str(train_sample)
credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]
credit_test
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
credit_train$default
credit_train$age
prop.table(table(credit_train$age))
names(credit)
?CrossTable
?knn
knn
pred
set.seed(30)
tr <- sample(data,945)
tr <- sample(nrow(data),945)
tr <- sample(1:nrow(data),945)
tr <- sample(1:nrow(data),945)
?knn.cv
library(class)
?knn.cv
?set.seed(30)
?minsplit
original_size
mean_face = colMeans(pic_mat)
mean_face_matrix = mean_face
dim(mean_face_matrix) = original_size
mean_face_pix = pixmapGrey(mean_face_matrix)
plot(mean_face_pix)
mydir = "D:/MP/Facial-Recognition-master"
setwd(mydir)
install.packages("pixmap")
library(pixmap)
######################################################################(a)
#Load the views P00A+000E+00, P00A+005E+10, P00A+005E-10, and P00A+010E+00 for all subjects.
#Convert each photo to a vector; store the collection as a matrix where each row is a photo.
# the list of pictures (note the absence of 14 means that 31 corresponds to yaleB32)
pic_list = c(1:38)
view_list = c( 'P00A+000E+00', 'P00A+005E+10', 'P00A+005E-10', 'P00A+010E+00')
dir_list_1 = dir(path="CroppedYale/",all.files=FALSE)
pic_data = vector("list",length(pic_list)*length(view_list)) # preallocate an empty list
pic_data_pgm = vector("list",length(pic_list)*length(view_list)) # preallocate an empty list to store the pgm for debugging
# Preallocate matrix to store picture vectors, store sizes for computations
this_face = read.pnm(file = "CroppedYale/yaleB01/yaleB01_P00A+010E+00.pgm")
this_face_matrix = getChannels(this_face)
original_size = dim(this_face_matrix)
pic_vector_length = prod(original_size)
pic_mat = mat.or.vec(length(pic_list)*length(view_list),pic_vector_length)
for ( i in 1:length(pic_list) ){
for ( j in 1:length(view_list) ){
# compile the correct file name
this_filename = sprintf("CroppedYale/%s/%s_%s.pgm", dir_list_1[pic_list[i]] , dir_list_1[pic_list[i]] , view_list[j])
this_face = read.pnm(file = this_filename)
this_face_matrix = getChannels(this_face)
# store pgm as element of the list
pic_data_pgm[[(i-1)*length(view_list)+j]] = this_face
# store matrix as element of the list
pic_data[[(i-1)*length(view_list)+j]] = this_face_matrix
# make the face into a vector and include in the data matrix
pic_mat[(i-1)*length(view_list)+j,] =  as.vector(this_face_matrix)
}
}
pic_mat_size = dim(pic_mat)
print(sprintf('The matrix of all faces has size %d by %d' , pic_mat_size[1] , pic_mat_size[2] ))
######################################################################a(b)
#Compute a "mean face," which is the average for each pixel across all of the faces.
#Subtract this off each of the faces. Display the mean face as a photo in the original size and save a copy as .png.
# Find the mean face vector
mean_face = colMeans(pic_mat)
# Now print it as a picture
mean_face_matrix = mean_face
dim(mean_face_matrix) = original_size
mean_face_pix = pixmapGrey(mean_face_matrix)
plot(mean_face_pix)
#Subtract off the mean face
pic_mat_centered = mat.or.vec(pic_mat_size[1],pic_mat_size[2])
for (i in 1:pic_mat_size[1]){
pic_mat_centered[i,] = pic_mat[i,] - mean_face
}
#####################################################################(c)
#Use prcomp() to find the principal components of your image matrix.
#Plot the number of components on the x-axis against the proportion of the variance explained on the y-axis.
pic_pca = prcomp(pic_mat_centered)
pic_pca
?prcomp
names(pic_pca)
pic_pca[152,1]
pic_pca[151,1]
pic_pca[5,152]
pic_pca$sdev
pic_pca$x
pic_pca$rotation
length(pic_pca$x[,1])
n_comp = length(pic_pca$x[,1])
pca_var = mat.or.vec(n_comp,1)
pca_var
n_comp
for (i in 1:n_comp){
if (i==1){
pca_var[i] = pic_pca$sdev[i]^2
#sdev c'est l'a standard derivation'ecart type dans PCA
}else{
pca_var[i] = pca_var[i-1] + pic_pca$sdev[i]^2
}
}
pca_var
pca_var = pca_var/pca_var[n_comp]*100
pca_var
pca_var = pca_var/pca_var[n_comp]*100
plot(pca_var,xlab="Numbre de composants",ylab="Percentage de la Variance")
this_face_row = vector()
for (i in 1:ind){
# Make the eigenface vector into a matrix
#pic_pca$rotation[,i] chaque colonne contienne un visage propre
this_eigenface = pic_pca$rotation[,i]
dim(this_eigenface) = original_size
this_face_row = cbind(this_face_row,this_eigenface)
if ((i %% 3)==0){
# make a new row
eigenface_mat = rbind(eigenface_mat,this_face_row)
# clear row vector
this_face_row = vector()
}
}
eigenface_mat = vector()
ind = 9
this_face_row = vector()
for (i in 1:ind){
# Make the eigenface vector into a matrix
#pic_pca$rotation[,i] chaque colonne contienne un visage propre
this_eigenface = pic_pca$rotation[,i]
dim(this_eigenface) = original_size
this_face_row = cbind(this_face_row,this_eigenface)
if ((i %% 3)==0){
# make a new row
eigenface_mat = rbind(eigenface_mat,this_face_row)
# clear row vector
this_face_row = vector()
}
}
eigenface_pgm = pixmapGrey((eigenface_mat-min(eigenface_mat))/(max(eigenface_mat)-min(eigenface_mat)))
plot(eigenface_pgm)
pic_data_pgm
pic_data_pgm[20]
plot(pic_data_pgm[20])
pixmapGrey(mean_face_matrix)
x=pixmapGrey(pic_data_pgm[20])
x=pixmapGrey(pic_data_pgm)
seq(1,1,5)
?seq
face_index = 20
eigenface_add_by_face <- function(face_index, max_faces, by_faces, mean_face, pic_pca, original_size){
# Initialize matrix for faces added in one eigenface at a time
face_by_eig_mat = vector()
face_by_eig_row = vector()
face_by_eig_vector = mean_face
# Store the temporary face as a matrix
face_temp = face_by_eig_vector
dim(face_temp) = original_size
face_by_eig_row = cbind(face_by_eig_row,face_temp)
# Now add in the eigenfaces
for (i in 1:24){
# Find the indices of the eigenfaces to include
ind_include = seq((i-1)*by_faces+1,i*by_faces,1)
# Add up the vector that is score[j] x eigenface[j]
eigenface_add = mat.or.vec(length(mean_face),1)
for (j in 1:length(ind_include)){
ind_temp = ind_include[j]
eigenface_add = eigenface_add + pic_pca$x[face_index,ind_temp]*pic_pca$rotation[,ind_temp]
}
face_by_eig_vector = face_by_eig_vector + eigenface_add
# Transform this back to matrix and include
face_temp = face_by_eig_vector
dim(face_temp) = original_size
face_by_eig_row = cbind(face_by_eig_row,face_temp)
if ((i %% 5) == 4){
# Start a new row
face_by_eig_mat = rbind(face_by_eig_mat,face_by_eig_row)
face_by_eig_row = vector()
}
}
# Return the matrix of faces
return(face_by_eig_mat)
}
max_faces = 24
ind_include
pic_pca$x[152,1]
pic_pca$rotation[,1]
# Now add in the eigenfaces
for (i in 1:24){
# Find the indices of the eigenfaces to include
ind_include = seq((i-1)*by_faces+1,i*by_faces,1)
# Add up the vector that is score[j] x eigenface[j]
eigenface_add = mat.or.vec(length(mean_face),1)
for (j in 1:length(ind_include)){
ind_temp = ind_include[j]
eigenface_add = eigenface_add + pic_pca$x[face_index,ind_temp]*pic_pca$rotation[,ind_temp]
}
face_by_eig_vector = face_by_eig_vector + eigenface_add
# Transform this back to matrix and include
face_temp = face_by_eig_vector
dim(face_temp) = original_size
face_by_eig_row = cbind(face_by_eig_row,face_temp)
if ((i %% 5) == 4){
# Start a new row
face_by_eig_mat = rbind(face_by_eig_mat,face_by_eig_row)
face_by_eig_row = vector()
}
}
# Return the matrix of faces
return(face_by_eig_mat)
}
# Loop through the first 24 eigenfaces
max_faces = 24
by_faces = 1
face_by_1 = eigenface_add_by_face(face_index, max_faces, by_faces, mean_face, pic_pca, original_size)
face_by_1_pm = pixmapGrey(face_by_1)
# Plot results
plot(face_by_1_pm)
# Loop through the first 120 eigenfaces
max_faces = 120
by_faces = 5
face_by_5 = eigenface_add_by_face(face_index, max_faces, by_faces, mean_face, pic_pca, original_size)
face_by_5_pm = pixmapGrey(face_by_5)
# Plot results
plot(face_by_5_pm)
pic_mat_mod = pic_mat[setdiff(1:pic_mat_size[1],1:4),]
pic_mat_mod_size = dim(pic_mat_mod)
pic_mat_mod_size
mean_face_mod = colMeans(pic_mat_mod)
pic_mat_mod_centered = mat.or.vec(pic_mat_mod_size[1],pic_mat_mod_size[2])
for (i in 1:pic_mat_mod_size[1]){
pic_mat_mod_centered[i,] = pic_mat_mod[i,] - mean_face_mod
}
query_pic = pic_mat[4,]
query_pic
query_pic = pic_mat[4,]
query_pic_centered = query_pic - mean_face_mod
pic_pca_mod = prcomp(pic_mat_mod_centered)
num_comp_mod = length(pic_pca_mod$x[,1])
num_comp_mod
?%*%
query_pic = pic_mat[4,]
query_pic_centered = query_pic - mean_face_mod
pic_pca_mod = prcomp(pic_mat_mod_centered)
num_comp_mod = length(pic_pca_mod$x[,1])
scores_query = mat.or.vec(num_comp_mod,1)
for (i in 1:num_comp_mod){   #148
loading_temp = pic_pca_mod$rotation[,i]
scores_query[i] = loading_temp %*% query_pic_centered
}
scores_query[1]
mydir = "D:/MP/Facial-Recognition-master"
setwd(mydir)
library(pixmap)
pic_list = c(1:38)  # liste de taille 38
view_list = c( 'P00A+000E+00', 'P00A+005E+10', 'P00A+005E-10', 'P00A+010E+00')  # les quatre vues
dir_list_1 = dir(path="CroppedYale/",all.files=FALSE)       # tous les fichie dans le dossier CroppedYale
pic_data = vector("list",length(pic_list)*length(view_list))   # pic_data c'est UN VECTEUR DE taille 152= 38*4
pic_data_pgm = vector("list",length(pic_list)*length(view_list))  # vecteur null
this_face = read.pnm(file = "CroppedYale/yaleB01/yaleB01_P00A+010E+00.pgm") # lire affiche une image
plot(this_face)
